% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gibbs_vnpc.R
\name{gibbs_vnpc_avg}
\alias{gibbs_vnpc_avg}
\title{Gibbs sampler for multivariate Bayesian inference with the nonparametrically corrected VAR likelihood}
\usage{
gibbs_vnpc_avg(
  data,
  samp_freq = 2 * pi,
  trunc_freq = 2 * pi,
  seg_n = 1,
  truncation = FALSE,
  trunc_freq_lim = NULL,
  corrected = FALSE,
  var.order = NULL,
  Ntotal,
  burnin,
  thin = 1,
  print_interval = 100,
  numerical_thresh = 1e-12,
  adaption.N = burnin,
  adaption.batchSize = 50,
  adaption.tar = 0.44,
  eta = ncol(data),
  omega_fun = create_omega_fun_from_beta_density(1, 1, 1),
  Sigma_fun = my_Sigma_fun,
  k.theta = 0.01,
  kmax = 100 * coars + 500 * (!coars),
  trunc_l = 0.1,
  trunc_r = 0.9,
  coars = F,
  L = 20,
  mu_beta = NULL,
  V_beta = NULL,
  sqrt_d = F
)
}
\arguments{
\item{data}{numerical matrix}

\item{samp_freq}{sampling frequency (default = 2*pi)}

\item{trunc_freq}{sampling frequency for truncated data (default = 2*pi)}

\item{seg_n}{number of segments (integer >= 1)}

\item{truncation}{flag indicating whether the data needs to be truncated (default = FALSE).}

\item{trunc_freq_lim}{frequency bounds of the truncated data, an positive integer or a 2-dimensional vector with lower and upper bounds being integers, used for truncation = TRUE only.}

\item{corrected}{flag indicating whether the corrected likelihood is used (default = FALSE).}

\item{var.order}{VAR order for the parametric working model}

\item{Ntotal}{total number of iterations to run the Markov chain}

\item{burnin}{number of initial iterations to be discarded}

\item{thin}{thinning number (postprocessing)}

\item{print_interval}{number of iterations, after which a status is printed to console}

\item{numerical_thresh}{lower (numerical pointwise) bound for the eigenvalues of the spectral density}

\item{adaption.N}{total number of iterations, in which the proposal variances (of r, U and VAR coefficients) are adapted}

\item{adaption.batchSize}{batch size of proposal adaption}

\item{adaption.tar}{target acceptance rate for adapted parameters}

\item{eta}{AGamma process parameter, real number > ncol(data) - 1}

\item{omega_fun}{AGamma process parameter, positive constant}

\item{Sigma_fun}{AGamma process parameter, Hpd matrix}

\item{k.theta}{prior parameter for polynomial degree k (propto exp(-k.theta*k*log(k)))}

\item{kmax}{upper bound for polynomial degree of Bernstein-Dirichlet mixture (can be set to Inf, algorithm is faster with kmax<Inf due to pre-computation of basis functions, but values 500<kmax<Inf are very memory intensive)}

\item{trunc_l, trunc_r}{left and right truncation of Bernstein polynomial basis functions, 0<=trunc_l<trunc_r<=1}

\item{coars}{flag indicating whether coarsened or default bernstein polynomials are used (see Appendix E.1 in Ghosal and van der Vaart 2017)}

\item{L}{truncation parameter of Gamma process}

\item{mu_beta}{prior parameter for VAR coefficients, stacked numerical vector}

\item{V_beta}{prior parameter for VAR coefficients, Hpd matrix}

\item{sqrt_d}{flag indicating whether the regular square root of a Hermitian matrix is used, otherwise the Cholesky decomposition is used to approximate the square root (see Remark 5.2 in Liu (2023))}
}
\value{
list containing the following fields:
 \item{data}{(centerd) data}
 \item{psd.median, psd.mean}{(pointwise) posterior median and mean}
 \item{psd.p05, psd.p95}{90\% pointwise credible interval}
 \item{psd.u05, psd.u95}{90\% uniform credible interval}
 \item{coherence.median}{(pointwise) posterior median coherence}
 \item{coherence.p05, coherence.p95}{(pointwise) 90\% pointwise credible interval for the coherence}
}
\description{
Obtain samples of the posterior of the multivariate corrected likelihood in conjuction with an Hpd AGamma process prior on the spectral density matrix
}
\references{
Y. Liu (2023)
\emph{A Nonparametrically corrected likelihood for Bayesian spectral analysis of multivariate time series}
PhD thesis, University of Auckland
<https://hdl.handle.net/2292/65154>
}
